{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reassign Session IDs and Recalculate Summary_Sessions\n",
        "\n",
        "This notebook:\n",
        "1. Groups records by (user_id, test_date, boyfriend_name) combination\n",
        "2. Assigns the same session_id to all records with matching combinations across all CSV files\n",
        "3. Recalculates Summary_Sessions based on the updated data\n",
        "\n",
        "**Use this when:**\n",
        "- Session IDs are inconsistent across tables\n",
        "- You need to fix session ID assignments based on user_id, test_date, and boyfriend_name\n",
        "- You want to recalculate Summary_Sessions from scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from decimal import Decimal\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent.parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.utils.session_id_generator import generate_session_id\n",
        "from src.utils.constants import DATE_FORMAT, CSV_SEPARATOR\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = project_root / \"data\"\n",
        "CSV_FILES = {\n",
        "    \"session_responses\": \"session_responses.csv\",\n",
        "    \"session_gtk_responses\": \"session_gtk_responses.csv\",\n",
        "    \"session_feedback\": \"session_feedback.csv\",\n",
        "    \"session_toxicity_rating\": \"session_toxicity_rating.csv\",\n",
        "    \"session_insights\": \"session_insights.csv\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_session_id_with_date(user_id: str, boyfriend_name: str, test_date: str) -> int:\n",
        "    \"\"\"\n",
        "    Generate a unique, deterministic session_id based on user_id, boyfriend_name, and test_date.\n",
        "    \n",
        "    Args:\n",
        "        user_id: Unique identifier for the user\n",
        "        boyfriend_name: Name of the boyfriend being rated\n",
        "        test_date: Test date (session date)\n",
        "        \n",
        "    Returns:\n",
        "        A positive integer session_id\n",
        "    \"\"\"\n",
        "    # Normalize inputs\n",
        "    user_id_norm = str(user_id).lower().strip()\n",
        "    bf_name_norm = str(boyfriend_name).lower().strip()\n",
        "    test_date_norm = str(test_date).strip()\n",
        "    \n",
        "    # Create a deterministic string from all three components\n",
        "    combined = f\"{user_id_norm}_{bf_name_norm}_{test_date_norm}\"\n",
        "    \n",
        "    # Generate hash using SHA256\n",
        "    hash_obj = hashlib.sha256(combined.encode('utf-8'))\n",
        "    hash_hex = hash_obj.hexdigest()\n",
        "    \n",
        "    # Convert first 8 characters of hash to integer\n",
        "    session_id = int(hash_hex[:8], 16) % (2**31 - 1)\n",
        "    \n",
        "    # Ensure it's positive and at least 1\n",
        "    if session_id == 0:\n",
        "        session_id = 1\n",
        "    \n",
        "    return session_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_test_date_column(df: pd.DataFrame) -> str:\n",
        "    \"\"\"Determine which column contains the test date.\"\"\"\n",
        "    # Check common column names for test date (in priority order)\n",
        "    # Prefer session_start_time as it's more consistent across tables\n",
        "    date_columns = ['session_start_time', 'test_date', 'timestamp']\n",
        "    for col in date_columns:\n",
        "        if col in df.columns:\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "def normalize_date(date_value):\n",
        "    \"\"\"Normalize date to string format using consistent DATE_FORMAT.\"\"\"\n",
        "    if pd.isna(date_value):\n",
        "        return None\n",
        "    # Convert to string and strip\n",
        "    date_str = str(date_value).strip()\n",
        "    # If it's a datetime object, format it consistently\n",
        "    try:\n",
        "        if isinstance(date_value, pd.Timestamp):\n",
        "            return date_value.strftime(DATE_FORMAT)\n",
        "        # Try parsing if it's a string and reformat\n",
        "        parsed_date = pd.to_datetime(date_str)\n",
        "        return parsed_date.strftime(DATE_FORMAT)\n",
        "    except:\n",
        "        return date_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_csv_separator(file_path):\n",
        "    \"\"\"Detect CSV separator by reading first line.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            first_line = f.readline()\n",
        "            # Count semicolons and commas\n",
        "            semicolon_count = first_line.count(';')\n",
        "            comma_count = first_line.count(',')\n",
        "            # Use semicolon if it appears, otherwise comma\n",
        "            if semicolon_count > 0:\n",
        "                return ';'\n",
        "            elif comma_count > 0:\n",
        "                return ','\n",
        "            else:\n",
        "                return CSV_SEPARATOR  # Default\n",
        "    except:\n",
        "        return CSV_SEPARATOR  # Default fallback\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"Load all CSV files and prepare them for session ID assignment.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Loading CSV Files\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    data = {}\n",
        "    \n",
        "    for table_name, filename in CSV_FILES.items():\n",
        "        file_path = DATA_DIR / filename\n",
        "        if not file_path.exists():\n",
        "            print(f\"[WARNING] File not found: {filename}\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            # Detect separator for each file\n",
        "            separator = detect_csv_separator(file_path)\n",
        "            df = pd.read_csv(file_path, sep=separator)\n",
        "            print(f\"[OK] Loaded {filename}: {len(df)} records (separator: '{separator}')\")\n",
        "            data[table_name] = df\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to load {filename}: {e}\")\n",
        "    \n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_session_mapping(data: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Create a mapping of (user_id, test_date, boyfriend_name) -> session_id.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping (user_id, test_date, boyfriend_name) tuples to session_id\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Creating Session ID Mapping\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    session_mapping = {}\n",
        "    \n",
        "    # Process each table to extract unique combinations\n",
        "    for table_name, df in data.items():\n",
        "        if df.empty:\n",
        "            continue\n",
        "        \n",
        "        # Get required columns - check with case-insensitive matching\n",
        "        user_id_col = None\n",
        "        boyfriend_name_col = None\n",
        "        \n",
        "        for col in df.columns:\n",
        "            col_lower = col.lower().strip()\n",
        "            if col_lower == 'user_id':\n",
        "                user_id_col = col\n",
        "            elif col_lower == 'boyfriend_name':\n",
        "                boyfriend_name_col = col\n",
        "        \n",
        "        if not user_id_col or not boyfriend_name_col:\n",
        "            print(f\"[WARNING] {table_name} missing required columns. Available columns: {list(df.columns[:10])}\")\n",
        "            continue\n",
        "        \n",
        "        # Get test_date column\n",
        "        date_col = get_test_date_column(df)\n",
        "        if not date_col:\n",
        "            print(f\"[WARNING] {table_name} has no date column (test_date, session_start_time, or timestamp)\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\nProcessing {table_name}...\")\n",
        "        \n",
        "        # Extract unique combinations\n",
        "        for idx, row in df.iterrows():\n",
        "            user_id = str(row[user_id_col]).strip()\n",
        "            boyfriend_name = str(row[boyfriend_name_col]).strip()\n",
        "            test_date = normalize_date(row[date_col])\n",
        "            \n",
        "            if not user_id or not boyfriend_name or not test_date:\n",
        "                print(f\"[WARNING] Row {idx} in {table_name} has missing values, skipping\")\n",
        "                continue\n",
        "            \n",
        "            key = (user_id, test_date, boyfriend_name)\n",
        "            \n",
        "            # Generate session_id if not already in mapping\n",
        "            if key not in session_mapping:\n",
        "                session_id = generate_session_id_with_date(user_id, boyfriend_name, test_date)\n",
        "                session_mapping[key] = session_id\n",
        "                print(f\"  Created mapping: ({user_id[:8]}..., {test_date[:10]}..., {boyfriend_name}) -> {session_id}\")\n",
        "    \n",
        "    print(f\"\\n[OK] Created {len(session_mapping)} unique session mappings\")\n",
        "    return session_mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_session_ids(data: dict, session_mapping: dict):\n",
        "    \"\"\"Update session IDs in all dataframes based on the mapping.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Updating Session IDs\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    updated_data = {}\n",
        "    \n",
        "    for table_name, df in data.items():\n",
        "        if df.empty:\n",
        "            updated_data[table_name] = df\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\nUpdating {table_name}...\")\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Get required columns - check with case-insensitive matching\n",
        "        user_id_col = None\n",
        "        boyfriend_name_col = None\n",
        "        \n",
        "        for col in df.columns:\n",
        "            col_lower = col.lower().strip()\n",
        "            if col_lower == 'user_id':\n",
        "                user_id_col = col\n",
        "            elif col_lower == 'boyfriend_name':\n",
        "                boyfriend_name_col = col\n",
        "        \n",
        "        if not user_id_col or not boyfriend_name_col:\n",
        "            print(f\"[WARNING] {table_name} missing required columns, skipping. Available: {list(df.columns[:10])}\")\n",
        "            updated_data[table_name] = df\n",
        "            continue\n",
        "        \n",
        "        # Get test_date column\n",
        "        date_col = get_test_date_column(df)\n",
        "        if not date_col:\n",
        "            print(f\"[WARNING] {table_name} has no date column, skipping\")\n",
        "            updated_data[table_name] = df\n",
        "            continue\n",
        "        \n",
        "        # Update IDs\n",
        "        updated_count = 0\n",
        "        skipped_count = 0\n",
        "        \n",
        "        for idx, row in df.iterrows():\n",
        "            user_id = str(row[user_id_col]).strip()\n",
        "            boyfriend_name = str(row[boyfriend_name_col]).strip()\n",
        "            test_date = normalize_date(row[date_col])\n",
        "            \n",
        "            if not user_id or not boyfriend_name or not test_date:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "            \n",
        "            key = (user_id, test_date, boyfriend_name)\n",
        "            \n",
        "            if key in session_mapping:\n",
        "                new_id = session_mapping[key]\n",
        "                old_id = row.get('id')\n",
        "                df.at[idx, 'id'] = new_id\n",
        "                if old_id != new_id:\n",
        "                    updated_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "        \n",
        "        print(f\"  Updated: {updated_count} records\")\n",
        "        if skipped_count > 0:\n",
        "            print(f\"  Skipped: {skipped_count} records (missing values)\")\n",
        "        \n",
        "        updated_data[table_name] = df\n",
        "    \n",
        "    return updated_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_updated_csvs(updated_data: dict, backup: bool = True):\n",
        "    \"\"\"Save updated dataframes to CSV files.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Saving Updated CSV Files\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if backup:\n",
        "        # Create backup directory\n",
        "        backup_dir = DATA_DIR / \"backup\"\n",
        "        backup_dir.mkdir(exist_ok=True)\n",
        "        print(f\"[INFO] Backups will be saved to: {backup_dir}\")\n",
        "    \n",
        "    for table_name, df in updated_data.items():\n",
        "        if table_name not in CSV_FILES:\n",
        "            continue\n",
        "        \n",
        "        filename = CSV_FILES[table_name]\n",
        "        file_path = DATA_DIR / filename\n",
        "        \n",
        "        # Backup original if requested\n",
        "        if backup and file_path.exists():\n",
        "            backup_path = backup_dir / f\"{filename}.backup\"\n",
        "            import shutil\n",
        "            shutil.copy2(file_path, backup_path)\n",
        "            print(f\"[OK] Backed up {filename}\")\n",
        "        \n",
        "        # Save updated file with consistent separator\n",
        "        try:\n",
        "            df.to_csv(file_path, sep=CSV_SEPARATOR, index=False)\n",
        "            print(f\"[OK] Saved {filename} ({len(df)} records)\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to save {filename}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recalculate_summary_sessions(data: dict):\n",
        "    \"\"\"Recalculate Summary_Sessions table from session_responses.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Recalculating Summary_Sessions\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if \"session_responses\" not in data:\n",
        "        print(\"[ERROR] session_responses not found in data\")\n",
        "        return None\n",
        "    \n",
        "    df = data[\"session_responses\"]\n",
        "    \n",
        "    if df.empty:\n",
        "        print(\"[WARNING] session_responses is empty\")\n",
        "        return None\n",
        "    \n",
        "    # Required columns\n",
        "    required_cols = ['toxic_score', 'filter_violations']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(f\"[ERROR] session_responses missing required columns: {required_cols}\")\n",
        "        return None\n",
        "    \n",
        "    # Convert to numeric\n",
        "    df['toxic_score'] = pd.to_numeric(df['toxic_score'], errors='coerce')\n",
        "    df['filter_violations'] = pd.to_numeric(df['filter_violations'], errors='coerce')\n",
        "    \n",
        "    # Remove rows with NaN values\n",
        "    df_clean = df.dropna(subset=['toxic_score', 'filter_violations'])\n",
        "    \n",
        "    if df_clean.empty:\n",
        "        print(\"[WARNING] No valid records after cleaning\")\n",
        "        return None\n",
        "    \n",
        "    # Calculate statistics\n",
        "    sum_toxic_score = float(df_clean['toxic_score'].sum())\n",
        "    max_toxic_score = float(df_clean['toxic_score'].max())\n",
        "    min_toxic_score = float(df_clean['toxic_score'].min())\n",
        "    avg_toxic_score = float(df_clean['toxic_score'].mean())\n",
        "    \n",
        "    sum_filter_violations = int(df_clean['filter_violations'].sum())\n",
        "    avg_filter_violations = float(df_clean['filter_violations'].mean())\n",
        "    \n",
        "    # Count unique sessions (unique id values)\n",
        "    count_guys = df_clean['id'].nunique()\n",
        "    \n",
        "    # Get max IDs from each table (for backward compatibility, but set to 0 as per code)\n",
        "    max_id_session_responses = 0\n",
        "    max_id_gtk_responses = 0\n",
        "    max_id_feedback = 0\n",
        "    max_id_session_toxicity_rating = 0\n",
        "    \n",
        "    last_update_date = datetime.now().strftime(DATE_FORMAT)\n",
        "    \n",
        "    summary_data = {\n",
        "        'summary_id': 1,\n",
        "        'sum_toxic_score': sum_toxic_score,\n",
        "        'max_toxic_score': max_toxic_score,\n",
        "        'min_toxic_score': min_toxic_score,\n",
        "        'avg_toxic_score': avg_toxic_score,\n",
        "        'sum_filter_violations': sum_filter_violations,\n",
        "        'avg_filter_violations': avg_filter_violations,\n",
        "        'count_guys': count_guys,\n",
        "        'max_id_session_responses': max_id_session_responses,\n",
        "        'max_id_gtk_responses': max_id_gtk_responses,\n",
        "        'max_id_feedback': max_id_feedback,\n",
        "        'max_id_session_toxicity_rating': max_id_session_toxicity_rating,\n",
        "        'last_update_date': last_update_date,\n",
        "    }\n",
        "    \n",
        "    print(f\"[OK] Calculated Summary_Sessions:\")\n",
        "    print(f\"  count_guys: {count_guys}\")\n",
        "    print(f\"  avg_toxic_score: {avg_toxic_score:.6f}\")\n",
        "    print(f\"  sum_toxic_score: {sum_toxic_score:.6f}\")\n",
        "    print(f\"  sum_filter_violations: {sum_filter_violations}\")\n",
        "    \n",
        "    return summary_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_summary_sessions(summary_data: dict):\n",
        "    \"\"\"Save Summary_Sessions to CSV.\"\"\"\n",
        "    if summary_data is None:\n",
        "        return\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Saving Summary_Sessions\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    file_path = DATA_DIR / \"Summary_Sessions.csv\"\n",
        "    \n",
        "    # Backup if exists\n",
        "    if file_path.exists():\n",
        "        backup_dir = DATA_DIR / \"backup\"\n",
        "        backup_dir.mkdir(exist_ok=True)\n",
        "        import shutil\n",
        "        backup_path = backup_dir / \"Summary_Sessions.csv.backup\"\n",
        "        shutil.copy2(file_path, backup_path)\n",
        "        print(f\"[OK] Backed up Summary_Sessions.csv\")\n",
        "    \n",
        "    # Create DataFrame and save with consistent separator\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    df.to_csv(file_path, sep=CSV_SEPARATOR, index=False)\n",
        "    print(f\"[OK] Saved Summary_Sessions.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_changes_summary(data: dict, updated_data: dict, session_mapping: dict):\n",
        "    \"\"\"Show a summary of changes before saving.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Changes Summary\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\nTotal unique sessions: {len(session_mapping)}\")\n",
        "    \n",
        "    print(\"\\nSession ID changes per table:\")\n",
        "    for table_name in CSV_FILES.keys():\n",
        "        if table_name not in data or table_name not in updated_data:\n",
        "            continue\n",
        "        \n",
        "        old_df = data[table_name]\n",
        "        new_df = updated_data[table_name]\n",
        "        \n",
        "        if old_df.empty or new_df.empty:\n",
        "            continue\n",
        "        \n",
        "        # Count changed IDs\n",
        "        if 'id' in old_df.columns and 'id' in new_df.columns:\n",
        "            changes = (old_df['id'] != new_df['id']).sum()\n",
        "            total = len(old_df)\n",
        "            print(f\"  {table_name}: {changes}/{total} records updated\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "def reassign_session_ids_and_recalculate(dry_run: bool = False):\n",
        "    \"\"\"\n",
        "    Main function to reassign session IDs and recalculate Summary_Sessions.\n",
        "    \n",
        "    Args:\n",
        "        dry_run: If True, only show what would be changed without saving files\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Reassign Session IDs and Recalculate Summary_Sessions\")\n",
        "    if dry_run:\n",
        "        print(\"DRY RUN MODE - No files will be modified\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Step 1: Load data\n",
        "    data = load_and_prepare_data()\n",
        "    \n",
        "    if not data:\n",
        "        print(\"[ERROR] No data loaded\")\n",
        "        return\n",
        "    \n",
        "    # Step 2: Create session mapping\n",
        "    session_mapping = create_session_mapping(data)\n",
        "    \n",
        "    if not session_mapping:\n",
        "        print(\"[ERROR] No session mappings created\")\n",
        "        return\n",
        "    \n",
        "    # Step 3: Update session IDs\n",
        "    updated_data = update_session_ids(data, session_mapping)\n",
        "    \n",
        "    # Step 4: Show summary\n",
        "    show_changes_summary(data, updated_data, session_mapping)\n",
        "    \n",
        "    if dry_run:\n",
        "        print(\"\\n[INFO] Dry run completed. No files were modified.\")\n",
        "        print(\"Set dry_run=False to apply changes.\")\n",
        "        return\n",
        "    \n",
        "    # Step 5: Save updated CSVs\n",
        "    save_updated_csvs(updated_data, backup=True)\n",
        "    \n",
        "    # Step 6: Recalculate Summary_Sessions\n",
        "    summary_data = recalculate_summary_sessions(updated_data)\n",
        "    \n",
        "    # Step 7: Save Summary_Sessions\n",
        "    save_summary_sessions(summary_data)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"[SUCCESS] All operations completed!\")\n",
        "    print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the reassignment process\n",
        "# First run with dry_run=True to preview changes, then set to False to apply\n",
        "# reassign_session_ids_and_recalculate(dry_run=True)  # Preview changes\n",
        "# reassign_session_ids_and_recalculate(dry_run=False)  # Apply changes\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
