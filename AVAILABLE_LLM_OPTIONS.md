# Available Free LLM Options

## üéØ Recommended Options

### 1. **Hugging Face Inference API** ‚≠ê‚≠ê‚≠ê (Best for Free Tier)
- **Website**: https://huggingface.co/
- **Free Tier**: 30,000 requests/month
- **Models Available**:
  - `mistralai/Mistral-7B-Instruct-v0.2` (Recommended)
  - `meta-llama/Llama-2-7b-chat-hf`
  - `mistralai/Mixtral-8x7B-Instruct-v0.1`
- **Setup**:
  1. Create free account at https://huggingface.co/
  2. Go to Settings ‚Üí Access Tokens
  3. Create new token (read permission)
  4. Set `HF_API_TOKEN` environment variable or add to `config/llm_credentials.txt`
- **Pros**:
  - ‚úÖ Truly free, no credit card required
  - ‚úÖ Generous free tier (30K requests/month)
  - ‚úÖ Many open-source models
  - ‚úÖ Easy to use
- **Cons**:
  - ‚ö†Ô∏è Can be slower than paid services
  - ‚ö†Ô∏è Rate limits on free tier

### 2. **Groq** ‚≠ê‚≠ê‚≠ê (Best for Speed)
- **Website**: https://console.groq.com/
- **Free Tier**: Very generous free tier
- **Models Available**:
  - `llama-3.1-8b-instant` (Recommended - Very Fast)
  - `llama-3.1-70b-versatile`
  - `mixtral-8x7b-32768`
  - `gemma-7b-it`
- **Setup**:
  1. Create free account at https://console.groq.com/
  2. Get API key from dashboard
  3. Install: `pip install groq`
  4. Set `GROQ_API_KEY` environment variable or add to `config/llm_credentials.txt`
- **Pros**:
  - ‚úÖ Extremely fast inference (often < 1 second)
  - ‚úÖ Good free tier
  - ‚úÖ Multiple model options
  - ‚úÖ Easy to use
- **Cons**:
  - ‚ö†Ô∏è Requires account creation
  - ‚ö†Ô∏è Rate limits (but generous)

## üîß Alternative Options

### 3. **Ollama** (Local - 100% Free)
- **Website**: https://ollama.ai/
- **Free**: Completely free, runs locally
- **Models**: Llama 2, Mistral, CodeLlama, etc.
- **Setup**: Install Ollama locally on your machine
- **Pros**:
  - ‚úÖ 100% free, no API limits
  - ‚úÖ Privacy-focused (runs locally)
  - ‚úÖ No internet required after setup
- **Cons**:
  - ‚ö†Ô∏è Requires local installation
  - ‚ö†Ô∏è Needs good hardware (GPU recommended)
  - ‚ö†Ô∏è Not suitable for production deployment
  - ‚ö†Ô∏è Requires additional integration code

### 4. **Together AI** (Free Credits)
- **Website**: https://together.ai/
- **Free Tier**: $25 free credits
- **Models**: Llama 2, Mistral, Mixtral
- **Pros**:
  - ‚úÖ Good free credits
  - ‚úÖ Multiple models
- **Cons**:
  - ‚ö†Ô∏è Requires credit card (but free tier available)
  - ‚ö†Ô∏è Credits expire

### 5. **Replicate** (Limited Free Tier)
- **Website**: https://replicate.com/
- **Free Tier**: Limited free tier
- **Models**: Various open-source models
- **Pros**:
  - ‚úÖ Easy to use
  - ‚úÖ Many models
- **Cons**:
  - ‚ö†Ô∏è Limited free tier
  - ‚ö†Ô∏è Requires credit card

## üìä Comparison Table

| Provider | Free Tier | Speed | Setup Difficulty | Credit Card | Best For |
|----------|-----------|-------|------------------|-------------|----------|
| **Hugging Face** | 30K/month | Medium | Easy | ‚ùå No | Production |
| **Groq** | Generous | ‚ö° Very Fast | Easy | ‚ùå No | Fast responses |
| **Ollama** | Unlimited | Fast (local) | Medium | ‚ùå No | Privacy/Offline |
| **Together AI** | $25 credits | Fast | Easy | ‚úÖ Yes | Testing |
| **Replicate** | Limited | Medium | Easy | ‚úÖ Yes | Testing |

## üöÄ Quick Start

### Hugging Face (Recommended)
```bash
# Set environment variable
export HF_API_TOKEN="your_token_here"

# Or create config/llm_credentials.txt:
# huggingface
# your_token_here
# mistralai/Mistral-7B-Instruct-v0.2
```

### Groq (Very Fast)
```bash
# Install package
pip install groq

# Set environment variable
export GROQ_API_KEY="your_key_here"

# Or create config/llm_credentials.txt:
# groq
# your_key_here
# llama-3.1-8b-instant
```

## üí° Recommendation

For this project, I recommend starting with **Hugging Face** because:
1. No credit card required
2. Generous free tier (30K requests/month)
3. Easy setup
4. Good for production use

If you need faster responses, switch to **Groq** (also free, very fast).

